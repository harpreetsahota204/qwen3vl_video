{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/harpreetsahota204/qwen3vl_video/blob/main/qwen3vl_fiftyone_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Qwen3-VL Video Model for FiftyOne\n",
        "\n",
        "This notebook demonstrates how to use the Qwen3-VL video understanding model with FiftyOne for:\n",
        "- Video description and analysis\n",
        "- Temporal event detection\n",
        "- OCR and object tracking\n",
        "- Video embeddings and similarity search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q fiftyone decord qwen-vl-utils transformers torch torchvision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "# Register the model source\n",
        "foz.register_zoo_model_source(\n",
        "    \"https://github.com/harpreetsahota204/qwen3vl_video\",\n",
        "    overwrite=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample video dataset\n",
        "dataset = foz.load_zoo_dataset(\"quickstart-video\", max_samples=5)\n",
        "\n",
        "# Compute metadata (required for temporal operations)\n",
        "dataset.compute_metadata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen3-VL model\n",
        "model = foz.load_zoo_model(\"Qwen/Qwen3-VL-8B-Instruct\")\n",
        "\n",
        "# Note: First load will download the model (~16GB)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Video Description\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.operation = \"description\"\n",
        "dataset.apply_model(model, label_field=\"description\", skip_failures=True)\n",
        "\n",
        "# View a sample description\n",
        "sample = dataset.first()\n",
        "print(sample.description_summary)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comprehensive Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.operation = \"comprehensive\"\n",
        "dataset.apply_model(model, label_field=\"analysis\", skip_failures=True)\n",
        "\n",
        "print(\"Analysis complete!\")\n",
        "print(f\"Fields added: {[f for f in dataset.get_field_schema().keys() if f.startswith('analysis')]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal Event Detection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.operation = \"temporal_localization\"\n",
        "dataset.apply_model(model, label_field=\"events\", skip_failures=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Video OCR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.operation = \"ocr\"\n",
        "dataset.apply_model(model, label_field=\"ocr\", skip_failures=True)\n",
        "\n",
        "print(\"OCR complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Video Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure pooling strategy\n",
        "model.pooling_strategy = \"mean\"\n",
        "\n",
        "# Compute embeddings\n",
        "dataset.compute_embeddings(\n",
        "    model,\n",
        "    embeddings_field=\"qwen_embeddings\",\n",
        "    skip_failures=True\n",
        ")\n",
        "\n",
        "print(\"Embeddings computed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Video Similarity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import fiftyone.brain as fob\n",
        "\n",
        "# Build similarity index\n",
        "fob.compute_similarity(\n",
        "    dataset,\n",
        "    brain_key=\"qwen_similarity\",\n",
        "    embeddings=\"qwen_embeddings\"\n",
        ")\n",
        "\n",
        "# Find similar videos\n",
        "query_sample = dataset.first()\n",
        "similar_view = dataset.sort_by_similarity(\n",
        "    query_sample,\n",
        "    k=5,\n",
        "    brain_key=\"qwen_similarity\"\n",
        ")\n",
        "\n",
        "print(f\"Found {len(similar_view)} similar videos\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## UMAP Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute UMAP visualization\n",
        "results = fob.compute_visualization(\n",
        "    dataset,\n",
        "    method=\"umap\",\n",
        "    brain_key=\"qwen_viz\",\n",
        "    embeddings=\"qwen_embeddings\",\n",
        "    num_dims=2\n",
        ")\n",
        "\n",
        "print(\"UMAP visualization computed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Launch FiftyOne App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset, auto=False)\n",
        "session.url\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Prompts (Optional)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.operation = \"custom\"\n",
        "model.custom_prompt = \"\"\"Analyze this video and provide:\n",
        "{\n",
        "  \"content_type\": \"educational/entertainment/promotional/other\",\n",
        "  \"mood\": \"calm/energetic/dramatic/neutral\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "dataset.apply_model(model, label_field=\"custom_analysis\", skip_failures=True)\n",
        "\n",
        "# View results\n",
        "sample = dataset.first()\n",
        "print(sample.custom_analysis_result)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
